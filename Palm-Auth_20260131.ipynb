{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ALIaSksADpyQ",
        "eT9vQW7J6LXj",
        "TUpkrFJS8juc",
        "El_J1GtQS-Rx",
        "3hnf4akZOOyQ"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamal94sm/Palm-Auth/blob/main/Palm-Auth_20260131.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading**"
      ],
      "metadata": {
        "id": "ALIaSksADpyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa1-nm2RerWl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CASIA_MS_Dataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.samples = []\n",
        "        self.hand_id_map = {}\n",
        "        self.domain_map = {}\n",
        "\n",
        "        hand_id_counter = 0\n",
        "        domain_counter = 0\n",
        "\n",
        "        for root, _, files in os.walk(data_path):\n",
        "            for fname in files:\n",
        "                if not fname.lower().endswith(\".jpg\"):\n",
        "                    continue\n",
        "\n",
        "                # Expected format: ID_{l|r}_{spectrum}_{iteration}.jpg\n",
        "                parts = fname[:-4].split(\"_\")\n",
        "                if len(parts) != 4:\n",
        "                    continue\n",
        "\n",
        "                subject_id, hand, spectrum, iteration = parts\n",
        "                hand_id = f\"{subject_id}_{hand}\"\n",
        "\n",
        "                if hand_id not in self.hand_id_map:\n",
        "                    self.hand_id_map[hand_id] = hand_id_counter\n",
        "                    hand_id_counter += 1\n",
        "\n",
        "                if spectrum not in self.domain_map:\n",
        "                    self.domain_map[spectrum] = domain_counter\n",
        "                    domain_counter += 1\n",
        "\n",
        "                img_path = os.path.join(root, fname)\n",
        "\n",
        "                self.samples.append((\n",
        "                    img_path,\n",
        "                    self.hand_id_map[hand_id],     # y_i (identity label)\n",
        "                    self.domain_map[spectrum]      # y_d (domain label)\n",
        "                ))\n",
        "\n",
        "        # ðŸ”¹ NEW: explicit domain-label list (for stratified splitting)\n",
        "        self.domain_labels = [y_d for _, _, y_d in self.samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, y_i, y_d = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_np = np.array(img)\n",
        "\n",
        "        # Resize\n",
        "        img_np = cv2.resize(img_np, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        img = torch.tensor(img_np, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        return img, y_i, y_d\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Dataset\n",
        "# ================================\n",
        "data_path = \"/content/drive/MyDrive/CASIA-MS-ROI\"\n",
        "dataset = CASIA_MS_Dataset(data_path)\n",
        "\n",
        "num_classes = len(dataset.hand_id_map)\n",
        "num_domains = len(dataset.domain_map)\n",
        "\n",
        "print(\"Total samples:\", len(dataset))\n",
        "print(\"Hand ID classes:\", num_classes)\n",
        "print(\"Domains:\", dataset.domain_map)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAPK3TU9e-Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Registration and Query sets**"
      ],
      "metadata": {
        "id": "1RnNzJqJuMww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ================================\n",
        "# Protocol parameters\n",
        "# ================================\n",
        "REG_RATIO = 0.75      # 50% for registration, 50% for query\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# ================================\n",
        "# Group samples by hand ID\n",
        "# ================================\n",
        "hand_to_indices = defaultdict(list)\n",
        "\n",
        "for idx, (_, y_i, _) in enumerate(dataset.samples):\n",
        "    hand_to_indices[y_i].append(idx)\n",
        "\n",
        "all_hand_ids = list(hand_to_indices.keys())\n",
        "num_total_hands = len(all_hand_ids)\n",
        "\n",
        "print(f\"Total hands: {num_total_hands}\")\n",
        "print(f\"Total samples: {len(dataset.samples)}\")\n",
        "\n",
        "# ================================\n",
        "# Split samples per hand (50/50)\n",
        "# Registration and Query sets share the same hands\n",
        "# ================================\n",
        "reg_idx = []\n",
        "qry_idx = []\n",
        "\n",
        "for hid in all_hand_ids:\n",
        "    indices = hand_to_indices[hid]\n",
        "    random.shuffle(indices)\n",
        "    split_point = int(len(indices) * REG_RATIO)\n",
        "\n",
        "    reg_idx.extend(indices[:split_point])\n",
        "    qry_idx.extend(indices[split_point:])\n",
        "\n",
        "# ================================\n",
        "# Sanity check\n",
        "# ================================\n",
        "def unique_hands(indices):\n",
        "    return set(dataset.samples[i][1] for i in indices)\n",
        "\n",
        "shared_hands = unique_hands(reg_idx).intersection(unique_hands(qry_idx))\n",
        "print(f\"Shared hands between registration and query: {len(shared_hands)} (expected: {num_total_hands})\")\n",
        "\n",
        "# ================================\n",
        "# Final splits\n",
        "# ================================\n",
        "splits = {\n",
        "    \"open_reg\": reg_idx,\n",
        "    \"open_query\": qry_idx\n",
        "}\n",
        "\n",
        "for k, v in splits.items():\n",
        "    print(f\"{k:10s}: {len(v):5d} samples | {len(unique_hands(v))} hands\")\n"
      ],
      "metadata": {
        "id": "z4UxRviFJpMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Analysis"
      ],
      "metadata": {
        "id": "eT9vQW7J6LXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fvcore"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ElF_-Ndb5W6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timm\n",
        "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
        "\n",
        "# 1. Load the official DeiT-Small model\n",
        "# deit_small_patch16_224 is the standard small variant\n",
        "model_deit = timm.create_model('deit_tiny_patch16_224', pretrained=True)\n",
        "model_deit.eval()\n",
        "\n",
        "# 2. Create dummy input (DeiT natively uses 224x224)\n",
        "dummy_input_deit = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# 3. Analyze FLOPs (Inference Cost)\n",
        "flops_deit = FlopCountAnalysis(model_deit, dummy_input_deit)\n",
        "total_flops_deit = flops_deit.total()\n",
        "\n",
        "# 4. Analyze Parameters\n",
        "total_params_deit = sum(p.numel() for p in model_deit.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"--- DeiT-Small Analysis (224x224) ---\")\n",
        "print(f\"Total Parameters: {total_params_deit / 1e6:.2f} Million\")\n",
        "print(f\"Inference Cost:   {total_flops_deit / 1e9:.4f} GFLOPs\")\n",
        "#print(\"\\nLayer-by-Layer Parameter Breakdown (Top Levels):\")\n",
        "#print(parameter_count_table(model_deit, max_depth=2))"
      ],
      "metadata": {
        "id": "RtQOctgV6ehl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Load DINOv2 ViT-S/14\n",
        "# ----------------------------\n",
        "model_dino = torch.hub.load(\n",
        "    \"facebookresearch/dinov2\",\n",
        "    \"dinov2_vits14\"\n",
        ").to(device)\n",
        "\n",
        "model_dino.eval()\n",
        "\n",
        "# ----------------------------\n",
        "# Dummy input (224Ã—224 RGB)\n",
        "# ----------------------------\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# FLOPs analysis\n",
        "# ----------------------------\n",
        "flops = FlopCountAnalysis(model_dino, dummy_input)\n",
        "total_flops = flops.total()\n",
        "\n",
        "# ----------------------------\n",
        "# Parameter count\n",
        "# ----------------------------\n",
        "total_params = sum(p.numel() for p in model_dino.parameters())\n",
        "\n",
        "# ----------------------------\n",
        "# Results\n",
        "# ----------------------------\n",
        "print(\"--- DINOv2 ViT-S/14 Analysis (224x224) ---\")\n",
        "print(f\"Total Parameters: {total_params / 1e6:.2f} M\")\n",
        "print(f\"Inference Cost:   {total_flops / 1e9:.4f} GFLOPs\")\n",
        "\n",
        "# Optional: layer-wise breakdown\n",
        "# print(parameter_count_table(model_dino, max_depth=2))\n"
      ],
      "metadata": {
        "id": "Fw9VXlyvmszb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Supervised Fine-tuning"
      ],
      "metadata": {
        "id": "TUpkrFJS8juc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained DeiT**"
      ],
      "metadata": {
        "id": "xtoX_LaiZZxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Self-Supervised Contrastive Fine-tuning (SSL) â€“ Single Cell\n",
        "# Backbone: DeiT (no identity labels, NT-Xent loss)\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Backbone (feature extractor)\n",
        "# ----------------------------\n",
        "backbone = timm.create_model(\n",
        "    \"deit_small_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=0\n",
        ").to(device)\n",
        "\n",
        "# Freeze most layers, unfreeze last (two) blocks\n",
        "for name, p in backbone.named_parameters():\n",
        "    p.requires_grad = False\n",
        "    if \"blocks.10\" in name or \"blocks.11\" in name:\n",
        "        p.requires_grad = True\n",
        "\n",
        "# ----------------------------\n",
        "# Projection Head (SSL only)\n",
        "# ----------------------------\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, in_dim, proj_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(self.net(x), dim=1)\n",
        "\n",
        "proj_head = ProjectionHead(backbone.num_features, 128).to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# NT-Xent (InfoNCE) Loss\n",
        "# ----------------------------\n",
        "def nt_xent_loss(z1, z2, temperature=0.1):\n",
        "    B = z1.size(0)\n",
        "    z = torch.cat([z1, z2], dim=0)              # [2B, D]\n",
        "    sim = torch.matmul(z, z.T) / temperature\n",
        "\n",
        "    labels = torch.arange(B, device=z.device)\n",
        "    labels = torch.cat([labels + B, labels])\n",
        "\n",
        "    mask = torch.eye(2 * B, device=z.device).bool()\n",
        "    sim.masked_fill_(mask, -1e9)\n",
        "\n",
        "    return F.cross_entropy(sim, labels)\n",
        "\n",
        "# ----------------------------\n",
        "# SSL Augmentations\n",
        "# ----------------------------\n",
        "ssl_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# Contrastive Dataset Wrapper\n",
        "# base_dataset must return (img, y_i, y_d)\n",
        "# ----------------------------\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, base_dataset, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _, _ = self.base_dataset[idx]  # ignore labels\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        v1 = self.transform(img)\n",
        "        v2 = self.transform(img)\n",
        "        return v1, v2\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset & Loader\n",
        "# ----------------------------\n",
        "# assumes: train_dataset already exists (CASIA subset for SSL)\n",
        "ssl_dataset = ContrastiveDataset(dataset, ssl_transform) ################################ dataset\n",
        "\n",
        "ssl_loader = DataLoader(\n",
        "    ssl_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Optimizer\n",
        "# ----------------------------\n",
        "optimizer = optim.AdamW(\n",
        "    list(filter(lambda p: p.requires_grad, backbone.parameters())) +\n",
        "    list(proj_head.parameters()),\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# SSL Training Loop\n",
        "# ----------------------------\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    backbone.train()\n",
        "    proj_head.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x1, x2 in tqdm(ssl_loader, desc=f\"Epoch {epoch+1} [SSL]\"):\n",
        "        x1 = x1.to(device)\n",
        "        x2 = x2.to(device)\n",
        "\n",
        "        h1 = backbone(x1)\n",
        "        h2 = backbone(x2)\n",
        "\n",
        "        z1 = proj_head(h1)\n",
        "        z2 = proj_head(h2)\n",
        "\n",
        "        loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | SSL Loss: {total_loss / len(ssl_loader):.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# IMPORTANT:\n",
        "# After SSL, discard projection head.\n",
        "# Use backbone embeddings for identification / verification.\n",
        "# ----------------------------\n",
        "backbone.eval()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TLWODr6pk1KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained DinoV2**"
      ],
      "metadata": {
        "id": "8WD1xNm6jIl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Self-Supervised Contrastive Fine-tuning (SSL) â€“ Single Cell\n",
        "# Backbone: DINOv2 ViT-S/14 (NT-Xent loss)\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Backbone (DINOv2)\n",
        "# ----------------------------\n",
        "# Loading dinov2_vits14 (Small version, patch size 14)\n",
        "backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "\n",
        "# DINOv2 Small embedding dimension is 384\n",
        "# (For ViT-B use 768, ViT-L use 1024)\n",
        "DINO_EMBED_DIM = 384\n",
        "\n",
        "# Freeze most layers, unfreeze last (two) blocks\n",
        "# In DINOv2 hub models, blocks are accessed via .blocks\n",
        "for name, p in backbone.named_parameters():\n",
        "    p.requires_grad = False\n",
        "    # Unfreezing the last two blocks (block 10 and 11 for ViT-S)\n",
        "    if \"blocks.10\" in name or \"blocks.11\" in name:\n",
        "        p.requires_grad = True\n",
        "\n",
        "# ----------------------------\n",
        "# Projection Head (SSL only)\n",
        "# ----------------------------\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, in_dim, proj_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(self.net(x), dim=1)\n",
        "\n",
        "# Using the explicit dimension for DINOv2\n",
        "proj_head = ProjectionHead(DINO_EMBED_DIM, 128).to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# NT-Xent (InfoNCE) Loss\n",
        "# ----------------------------\n",
        "def nt_xent_loss(z1, z2, temperature=0.1):\n",
        "    B = z1.size(0)\n",
        "    z = torch.cat([z1, z2], dim=0)               # [2B, D]\n",
        "    sim = torch.matmul(z, z.T) / temperature\n",
        "\n",
        "    labels = torch.arange(B, device=z.device)\n",
        "    labels = torch.cat([labels + B, labels])\n",
        "\n",
        "    mask = torch.eye(2 * B, device=z.device).bool()\n",
        "    sim.masked_fill_(mask, -1e9)\n",
        "\n",
        "    return F.cross_entropy(sim, labels)\n",
        "\n",
        "# ----------------------------\n",
        "# SSL Augmentations (Optimized for DINOv2)\n",
        "# ----------------------------\n",
        "ssl_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)), # Multiples of 14 preferred\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    # DINOv2 uses standard ImageNet normalization\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# Contrastive Dataset Wrapper\n",
        "# ----------------------------\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, base_dataset, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Adjusted to handle common dataset returns (img, label)\n",
        "        # or (img, label, other)\n",
        "        data = self.base_dataset[idx]\n",
        "        img = data[0]\n",
        "\n",
        "        if not isinstance(img, Image.Image):\n",
        "            img = transforms.ToPILImage()(img)\n",
        "\n",
        "        v1 = self.transform(img)\n",
        "        v2 = self.transform(img)\n",
        "        return v1, v2\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset & Loader\n",
        "# ----------------------------\n",
        "# Reusing your 'dataset' variable\n",
        "ssl_dataset = ContrastiveDataset(dataset, ssl_transform)\n",
        "\n",
        "ssl_loader = DataLoader(\n",
        "    ssl_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Optimizer\n",
        "# ----------------------------\n",
        "# Only optimize parameters with requires_grad=True\n",
        "optimizer = optim.AdamW(\n",
        "    list(filter(lambda p: p.requires_grad, backbone.parameters())) +\n",
        "    list(proj_head.parameters()),\n",
        "    lr=1e-5, # DINOv2 is sensitive; start with a lower LR than DeiT\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# SSL Training Loop\n",
        "# ----------------------------\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    backbone.train()\n",
        "    proj_head.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x1, x2 in tqdm(ssl_loader, desc=f\"Epoch {epoch+1} [DINOv2 SSL]\"):\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "\n",
        "        # DINOv2 hub model returns the CLS token features [Batch, 384]\n",
        "        h1 = backbone(x1)\n",
        "        h2 = backbone(x2)\n",
        "\n",
        "        z1 = proj_head(h1)\n",
        "        z2 = proj_head(h2)\n",
        "\n",
        "        loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | SSL Loss: {total_loss / len(ssl_loader):.4f}\")\n",
        "\n",
        "backbone.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "urgnmiYOjNsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Save the finetuned backbone\n",
        "# ----------------------------\n",
        "save_path = \"/content/drive/MyDrive/CASIA-MS-ROI/smallDinoV2_CASIA_SSL_contrastive.pth\"\n",
        "torch.save(backbone.state_dict(), save_path)\n",
        "print(f\"Finetuned backbone saved at: {save_path}\")"
      ],
      "metadata": {
        "id": "Hj4RNmtV2dwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Identification**"
      ],
      "metadata": {
        "id": "0GUP5rz-ZS6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backbone Models**"
      ],
      "metadata": {
        "id": "ehST3AP-tVkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "backbone_smalldeit = timm.create_model(\n",
        "    \"deit_small_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=0\n",
        ").to(device)\n",
        "\n",
        "backbone_tinydeit = timm.create_model(\n",
        "    \"deit_tiny_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=0\n",
        ").to(device)\n",
        "\n",
        "backbone_smalldino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)"
      ],
      "metadata": {
        "id": "hKTnmXT0uPZH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuned Models with SSL**"
      ],
      "metadata": {
        "id": "cWzwp2GwtNaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "### small Dino-V2\n",
        "# Re-instantiate the base model architecture\n",
        "# It must be the exact same model (dinov2_vits14)\n",
        "finetuned_smalldino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "\n",
        "# Load the saved weights\n",
        "save_path = \"/content/drive/MyDrive/CASIA-MS-ROI/smallDinoV2_CASIA_SSL_contrastive.pth\"\n",
        "state_dict = torch.load(save_path, map_location=device)\n",
        "finetuned_smalldino.load_state_dict(state_dict)\n",
        "finetuned_smalldino.to(device)\n",
        "finetuned_smalldino.eval()\n",
        "print(\"Finetuned DINOv2 loaded successfully.\")\n",
        "\n",
        "\n",
        "### tiny DeiT\n",
        "finetuned_tinydeit = timm.create_model(\n",
        "    \"deit_tiny_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=0\n",
        ").to(device)\n",
        "\n",
        "save_path2 = \"/content/drive/MyDrive/CASIA-MS-ROI/tinyDeiT_CASIA_SSL_contrastive.pth\"\n",
        "state_dict = torch.load(save_path2, map_location=device)\n",
        "finetuned_tinydeit.load_state_dict(state_dict)\n",
        "finetuned_tinydeit.to(device)\n",
        "finetuned_tinydeit.eval()\n",
        "print(\"finetuned_tinydeit loaded successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "### small DeiT\n",
        "finetuned_smalldeit = timm.create_model(\n",
        "    \"deit_small_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=0\n",
        ").to(device)\n",
        "\n",
        "save_path3 = \"/content/drive/MyDrive/CASIA-MS-ROI/smallDeiT_CASIA_SSL_contrastive.pth\"\n",
        "state_dict = torch.load(save_path3, map_location=device)\n",
        "finetuned_smalldeit.load_state_dict(state_dict)\n",
        "finetuned_smalldeit.to(device)\n",
        "finetuned_smalldeit.eval()\n",
        "print(\"finetuned_smalldeit loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "3ImeXGVziGPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CASIA**"
      ],
      "metadata": {
        "id": "BMXiiNAPWkIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_embeddings(model, loader):\n",
        "    feats = defaultdict(list)\n",
        "\n",
        "    for imgs, labels, _ in loader:\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        emb = model(imgs)          # (B, 192)\n",
        "        emb = F.normalize(emb, dim=1)\n",
        "\n",
        "        for e, l in zip(emb.cpu(), labels):\n",
        "            feats[l].append(e)\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # Crucial for DINOv2 feature extraction\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Create Subsets using precomputed indices\n",
        "reg_ds = Subset(dataset, reg_idx)\n",
        "qry_ds = Subset(dataset, qry_idx)\n",
        "\n",
        "# DataLoaders\n",
        "reg_loader = DataLoader(\n",
        "    reg_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "qry_loader = DataLoader(\n",
        "    qry_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Registration samples:\", len(reg_ds))\n",
        "print(\"Query samples:\", len(qry_ds))\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Extract embeddings (dictionary: label -> list of tensors)\n",
        "# ========================================\n",
        "reg_feats = extract_embeddings(backbone_tinydeit, reg_loader) ############################################# model/backbone\n",
        "qry_feats = extract_embeddings(backbone_tinydeit, qry_loader)\n",
        "\n",
        "# ========================================\n",
        "# Flatten features: one row per sample (no averaging)\n",
        "# ========================================\n",
        "def flatten_feats(feat_dict):\n",
        "    feat_list = []\n",
        "    label_list = []\n",
        "    for label, tensors in feat_dict.items():\n",
        "        for t in tensors:\n",
        "            feat_list.append(t)\n",
        "            label_list.append(label)\n",
        "    mat = torch.stack(feat_list)\n",
        "    mat = F.normalize(mat, dim=1)\n",
        "    return mat, label_list\n",
        "\n",
        "RegMat, reg_labels = flatten_feats(reg_feats)\n",
        "QryMat, qry_labels = flatten_feats(qry_feats)\n",
        "\n",
        "print(\"Registration matrix:\", RegMat.shape)  # e.g., (num_reg_samples, feat_dim)\n",
        "print(\"Query matrix:\", QryMat.shape)         # e.g., (num_query_samples, feat_dim)\n",
        "\n",
        "# ========================================\n",
        "# Cosine similarity: sample vs sample\n",
        "# ========================================\n",
        "sim = QryMat @ RegMat.T  # [num_query_samples, num_reg_samples]\n",
        "\n",
        "# ========================================\n",
        "# KNN (k = 5) classification\n",
        "# ========================================\n",
        "\n",
        "k = 1\n",
        "\n",
        "# Top-k similarities and indices\n",
        "topk_sim, topk_idx = torch.topk(sim, k=k, dim=1)  # [num_query, k]\n",
        "\n",
        "pred_labels = []\n",
        "\n",
        "for i in range(topk_idx.size(0)):\n",
        "    neighbor_labels = [reg_labels[j] for j in topk_idx[i].tolist()]\n",
        "    neighbor_sims   = topk_sim[i].tolist()\n",
        "\n",
        "    # Majority vote\n",
        "    label_votes = defaultdict(float)\n",
        "    for lbl, s in zip(neighbor_labels, neighbor_sims):\n",
        "        label_votes[lbl] += 1.0        # vote count\n",
        "        # label_votes[lbl] += s        # (optional) similarity-weighted vote\n",
        "\n",
        "    # Select label with highest vote\n",
        "    pred_label = max(label_votes.items(), key=lambda x: x[1])[0]\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Accuracy\n",
        "# ----------------------------\n",
        "accuracy = accuracy_score(qry_labels, pred_labels)\n",
        "print(f\"Sample-to-sample accuracy: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-CYiRzAYWnW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Palm-Auth dataset**"
      ],
      "metadata": {
        "id": "ZznUPmu5WfFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HandIDDataset(Dataset):\n",
        "    def __init__(self, root_dir, sessions, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "\n",
        "        participants = [f\"{i:02d}\" for i in range(1, 5)]\n",
        "        hands = {\"left\": \"l\", \"right\": \"r\"}\n",
        "\n",
        "        for pid in participants:\n",
        "            for s in sessions:\n",
        "                d = os.path.join(root_dir, f\"{pid}_{s}\")\n",
        "                if not os.path.isdir(d):\n",
        "                    continue\n",
        "\n",
        "                for fname in os.listdir(d):\n",
        "                    if not fname.lower().endswith(\".jpg\"):\n",
        "                        continue\n",
        "\n",
        "                    fname_lower = fname.lower()\n",
        "\n",
        "                    # robust hand detection\n",
        "                    hand_key = None\n",
        "                    for h in hands:\n",
        "                        if h in fname_lower:\n",
        "                            hand_key = h\n",
        "                            break\n",
        "\n",
        "                    if hand_key is None:\n",
        "                        continue  # skip files without left/right\n",
        "\n",
        "                    label = f\"{pid}_{hands[hand_key]}\"\n",
        "                    self.samples.append(\n",
        "                        (os.path.join(d, fname), label)\n",
        "                    )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n"
      ],
      "metadata": {
        "id": "hO-40KgLAO5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "\n",
        "##############################################################\n",
        "'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # Converts to grayscale but keeps 3 channels (R=G=B)\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    # Crucial for DINOv2 feature extraction\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_embeddings(model, loader):\n",
        "    model.eval() # Ensure model is in eval mode\n",
        "    feats = defaultdict(list)\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # DINOv2 Small returns (B, 384)\n",
        "        emb = model(imgs)\n",
        "        emb = F.normalize(emb, dim=1)\n",
        "\n",
        "        for e, l in zip(emb.cpu(), labels):\n",
        "            feats[l].append(e)\n",
        "\n",
        "    return feats\n",
        "\n",
        "# ========================================\n",
        "# Registration: S1 + S2\n",
        "# ========================================\n",
        "reg_ds = HandIDDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/palmprint_SIT/users-roi\",\n",
        "    sessions=[\"S3\", \"S1\"],\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Query: S3\n",
        "qry_ds = HandIDDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/palmprint_SIT/users-roi\",\n",
        "    sessions=[\"S2\"],\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "reg_loader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n",
        "qry_loader = DataLoader(qry_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "# ========================================\n",
        "# Extract embeddings (dictionary: label -> list of tensors)\n",
        "# ========================================\n",
        "reg_feats = extract_embeddings(finetuned_smalldino, reg_loader) #############################################\n",
        "qry_feats = extract_embeddings(finetuned_smalldino, qry_loader)\n",
        "\n",
        "# ========================================\n",
        "# Flatten features: one row per sample (no averaging)\n",
        "# ========================================\n",
        "def flatten_feats(feat_dict):\n",
        "    feat_list = []\n",
        "    label_list = []\n",
        "    for label, tensors in feat_dict.items():\n",
        "        for t in tensors:\n",
        "            feat_list.append(t)\n",
        "            label_list.append(label)\n",
        "    mat = torch.stack(feat_list)\n",
        "    mat = F.normalize(mat, dim=1)\n",
        "    return mat, label_list\n",
        "\n",
        "RegMat, reg_labels = flatten_feats(reg_feats)\n",
        "QryMat, qry_labels = flatten_feats(qry_feats)\n",
        "\n",
        "print(\"Registration matrix:\", RegMat.shape)  # e.g., (num_reg_samples, feat_dim)\n",
        "print(\"Query matrix:\", QryMat.shape)         # e.g., (num_query_samples, feat_dim)\n",
        "\n",
        "# ========================================\n",
        "# Cosine similarity: sample vs sample\n",
        "# ========================================\n",
        "sim = QryMat @ RegMat.T  # [num_query_samples, num_reg_samples]\n",
        "\n",
        "# ========================================\n",
        "# KNN classification\n",
        "# ========================================\n",
        "\n",
        "k = 1\n",
        "\n",
        "# Top-k similarities and indices\n",
        "topk_sim, topk_idx = torch.topk(sim, k=k, dim=1)  # [num_query, k]\n",
        "\n",
        "pred_labels = []\n",
        "\n",
        "for i in range(topk_idx.size(0)):\n",
        "    neighbor_labels = [reg_labels[j] for j in topk_idx[i].tolist()]\n",
        "    neighbor_sims   = topk_sim[i].tolist()\n",
        "\n",
        "    # Majority vote\n",
        "    label_votes = defaultdict(float)\n",
        "    for lbl, s in zip(neighbor_labels, neighbor_sims):\n",
        "        label_votes[lbl] += 1.0        # vote count\n",
        "        # label_votes[lbl] += s        # (optional) similarity-weighted vote\n",
        "\n",
        "    # Select label with highest vote\n",
        "    pred_label = max(label_votes.items(), key=lambda x: x[1])[0]\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Accuracy\n",
        "# ----------------------------\n",
        "accuracy = accuracy_score(qry_labels, pred_labels)\n",
        "print(f\"Sample-to-sample accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Confusion matrix (Normalized %)\n",
        "# ----------------------------\n",
        "labels_sorted = sorted(list(set(qry_labels + reg_labels)))\n",
        "\n",
        "# Use normalize='true' to get proportions (0 to 1) per row\n",
        "cm = confusion_matrix(qry_labels, pred_labels, labels=labels_sorted, normalize='true')\n",
        "\n",
        "# Convert proportions to percentages\n",
        "cm_percent = cm * 100\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "# fmt=\".1f\" displays one decimal point; adjust to \".2f\" for two\n",
        "sns.heatmap(cm_percent, annot=True, fmt=\".1f\", xticklabels=labels_sorted, yticklabels=labels_sorted, cmap=\"Blues\")\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Ground Truth\")\n",
        "plt.title(\"Confusion Matrix (Accuracy % per Class)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "84SWAB4fJFkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cross-session identification**"
      ],
      "metadata": {
        "id": "zRruoU4uXOQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#tint_factors = [0.6, 0.6, 1.2]\n",
        "tint_factors = [1.0, 1.0, 1.0]\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    # Apply the tint: Multiply each channel by its factor\n",
        "    transforms.Lambda(lambda x: x * torch.tensor(tint_factors).view(3, 1, 1)),\n",
        "    # Clamp values to [0, 1] to prevent overflow before normalization\n",
        "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# List of models to evaluate\n",
        "models_to_test = {\n",
        "    \"Backbone Tiny-DeiT\": backbone_tinydeit,\n",
        "    \"Backbone Small-DeiT\": backbone_smalldeit,\n",
        "    \"Backbone Small-Dino\": backbone_smalldino,\n",
        "    \"Finetuned Tiny-DeiT\": finetuned_tinydeit,\n",
        "    \"Finetuned Small-DeiT\": finetuned_smalldeit,\n",
        "    \"Finetuned Small-Dino\": finetuned_smalldino\n",
        "}\n",
        "\n",
        "# Session combinations: (Registration Sessions, Query Session)\n",
        "session_splits = [\n",
        "    ([\"S1\", \"S2\"], [\"S3\"]),\n",
        "    ([\"S1\", \"S3\"], [\"S2\"]),\n",
        "    ([\"S2\", \"S3\"], [\"S1\"])\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model in models_to_test.items():\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    row = {\"Model\": model_name}\n",
        "\n",
        "    for reg_sess, qry_sess in session_splits:\n",
        "        split_name = f\"Reg:{'+'.join(reg_sess)} / Qry:{qry_sess[0]}\"\n",
        "\n",
        "        # 1. Setup DataLoaders for this specific split\n",
        "        reg_ds = HandIDDataset(root_dir=\"/content/drive/MyDrive/palmprint_SIT/users-roi\",\n",
        "                               sessions=reg_sess, transform=transform)\n",
        "        qry_ds = HandIDDataset(root_dir=\"/content/drive/MyDrive/palmprint_SIT/users-roi\",\n",
        "                               sessions=qry_sess, transform=transform)\n",
        "\n",
        "        reg_loader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n",
        "        qry_loader = DataLoader(qry_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "        # 2. Extract and Flatten features\n",
        "        reg_feats = extract_embeddings(model, reg_loader)\n",
        "        qry_feats = extract_embeddings(model, qry_loader)\n",
        "\n",
        "        RegMat, reg_labels = flatten_feats(reg_feats)\n",
        "        QryMat, qry_labels = flatten_feats(qry_feats)\n",
        "\n",
        "        # 3. Compute Similarity and Preds (KNN k=1)\n",
        "        # Move to GPU for faster matrix multiplication if possible\n",
        "        RegMat, QryMat = RegMat.to(device), QryMat.to(device)\n",
        "        sim = QryMat @ RegMat.T\n",
        "\n",
        "        _, topk_idx = torch.topk(sim, k=1, dim=1)\n",
        "\n",
        "        # 4. Calculate Accuracy\n",
        "        pred_labels = [reg_labels[i.item()] for i in topk_idx.flatten()]\n",
        "        acc = accuracy_score(qry_labels, pred_labels) * 100\n",
        "\n",
        "        row[split_name] = f\"{acc:.2f}%\"\n",
        "\n",
        "    results.append(row)\n",
        "\n",
        "# ========================================\n",
        "# Output Table\n",
        "# ========================================\n",
        "df = pd.DataFrame(results)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Optional: Save to CSV\n",
        "# df.to_csv(\"model_evaluation_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "w2-73N1zpd4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leave-One-Spectrum-Out (LOSO) Evaluation"
      ],
      "metadata": {
        "id": "7Abpa4HxXX4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HandIDDataset(Dataset):\n",
        "    def __init__(self, root_dir, sessions, target_spectrums=None, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        # Your specific 9 spectrums\n",
        "        self.all_spectrums = [\"red\", \"green\", \"blue\", \"white\", \"ir\", \"magenta\", \"yellow\", \"pink\", \"orange\"]\n",
        "\n",
        "        # Adjust range based on your current participant count (e.g., 1 to 4)\n",
        "        participants = [f\"{i:02d}\" for i in range(1, 5)]\n",
        "        hands = {\"left\": \"l\", \"right\": \"r\"}\n",
        "\n",
        "        for pid in participants:\n",
        "            for s in sessions:\n",
        "                d = os.path.join(root_dir, f\"{pid}_{s}\")\n",
        "                if not os.path.isdir(d): continue\n",
        "\n",
        "                for fname in os.listdir(d):\n",
        "                    if not fname.lower().endswith(\".jpg\"): continue\n",
        "\n",
        "                    fname_lower = fname.lower()\n",
        "\n",
        "                    # 1. Detect Spectrum from filename (e.g., 'blue' in '01_S1_Left_blue.jpg')\n",
        "                    spec_found = None\n",
        "                    for spec in self.all_spectrums:\n",
        "                        if spec in fname_lower:\n",
        "                            spec_found = spec\n",
        "                            break\n",
        "\n",
        "                    if spec_found is None: continue\n",
        "\n",
        "                    # 2. Filter based on the target_spectrums list\n",
        "                    if target_spectrums is not None and spec_found not in target_spectrums:\n",
        "                        continue\n",
        "\n",
        "                    # 3. Detect Hand (robust check for 'left' or 'right')\n",
        "                    hand_key = None\n",
        "                    for h in hands:\n",
        "                        if h in fname_lower:\n",
        "                            hand_key = h\n",
        "                            break\n",
        "\n",
        "                    if hand_key:\n",
        "                        label = f\"{pid}_{hands[hand_key]}\"\n",
        "                        self.samples.append((os.path.join(d, fname), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define models to evaluate\n",
        "models_dict = {\n",
        "    \"Backbone Tiny-DeiT\": backbone_tinydeit,\n",
        "    \"Backbone Small-DeiT\": backbone_smalldeit,\n",
        "    \"Backbone Small-Dino\": backbone_smalldino,\n",
        "    \"Finetuned Tiny-DeiT\": finetuned_tinydeit,\n",
        "    \"Finetuned Small-DeiT\": finetuned_smalldeit,\n",
        "    \"Finetuned Small-Dino\": finetuned_smalldino\n",
        "}\n",
        "\n",
        "target_spectrums = [\"red\", \"green\", \"blue\", \"white\", \"ir\", \"magenta\", \"yellow\", \"pink\", \"orange\"]\n",
        "results = []\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    model_results = {\"Model\": name}\n",
        "\n",
        "    print(f\"Evaluating {name}...\")\n",
        "\n",
        "    for qry_spec in target_spectrums:\n",
        "        # Reg = {All 9 spectrums except current one}, Que = {Current spectrum}\n",
        "        reg_specs = [s for s in target_spectrums if s != qry_spec]\n",
        "\n",
        "        # Load datasets (using S1, S2, and S3 for maximum gallery coverage)\n",
        "        reg_ds = HandIDDataset(root_dir=\"/content/drive/MyDrive/palmprint_SIT/users-roi\",\n",
        "                               sessions=[\"S1\", \"S2\", \"S3\"], target_spectrums=reg_specs, transform=transform)\n",
        "        qry_ds = HandIDDataset(root_dir=\"/content/drive/MyDrive/palmprint_SIT/users-roi\",\n",
        "                               sessions=[\"S1\", \"S2\", \"S3\"], target_spectrums=[qry_spec], transform=transform)\n",
        "\n",
        "        if len(reg_ds) == 0 or len(qry_ds) == 0:\n",
        "            model_results[f\"Que:{qry_spec}\"] = \"N/A\"\n",
        "            continue\n",
        "\n",
        "        reg_loader = DataLoader(reg_ds, batch_size=32, shuffle=False)\n",
        "        qry_loader = DataLoader(qry_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "        # Extract features\n",
        "        reg_feats = extract_embeddings(model, reg_loader)\n",
        "        qry_feats = extract_embeddings(model, qry_loader)\n",
        "\n",
        "        RegMat, reg_labels = flatten_feats(reg_feats)\n",
        "        QryMat, qry_labels = flatten_feats(qry_feats)\n",
        "\n",
        "        # Similarity Matching\n",
        "        sim = QryMat.to(device) @ RegMat.to(device).T\n",
        "        topk_idx = torch.topk(sim, k=1, dim=1)[1]\n",
        "\n",
        "        pred_labels = [reg_labels[idx.item()] for idx in topk_idx.flatten()]\n",
        "        acc = accuracy_score(qry_labels, pred_labels) * 100\n",
        "\n",
        "        model_results[f\"Que:{qry_spec}\"] = f\"{acc:.1f}%\"\n",
        "\n",
        "    results.append(model_results)\n",
        "\n",
        "# Create and display final comparison table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n--- Cross-Spectral Evaluation Table ---\")\n",
        "print(df_results.to_string(index=False))"
      ],
      "metadata": {
        "id": "M72kexqRweBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**importance of spectrums**"
      ],
      "metadata": {
        "id": "QrZFyZn9-sxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ================================\n",
        "# Session-based Splitting (S1-S4 for Reg, S5-S6 for Qry)\n",
        "# ================================\n",
        "reg_idx = []\n",
        "qry_idx = []\n",
        "\n",
        "# Map to store info for voting later: { hand_id: {session_id: {spectrum: sample_index}} }\n",
        "hand_session_groups = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "for idx, (path, y_i, _) in enumerate(dataset.samples):\n",
        "    filename = os.path.basename(path) # e.g., 001_l_460_06 (1).jpg\n",
        "    parts = filename.split('_')\n",
        "\n",
        "    if len(parts) < 4:\n",
        "        continue # Skip malformed filenames\n",
        "\n",
        "    spec = parts[2] # e.g., '460'\n",
        "\n",
        "    # Robust session parsing: extract digits from the session part\n",
        "    sess_str = parts[3].split('.')[0]\n",
        "    sess_match = re.search(r'\\d+', sess_str)\n",
        "\n",
        "    if sess_match:\n",
        "        sess = int(sess_match.group())\n",
        "\n",
        "        # Store for majority vote logic\n",
        "        hand_session_groups[y_i][sess][spec] = idx\n",
        "\n",
        "        # Assign to Registration or Query index lists\n",
        "        if 1 <= sess <= 4:\n",
        "            reg_idx.append(idx)\n",
        "        elif 5 <= sess <= 6:\n",
        "            qry_idx.append(idx)\n",
        "\n",
        "print(f\"Reg samples (S1-S4): {len(reg_idx)} | Qry samples (S5-S6): {len(qry_idx)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "G_k1uR2Z-sYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter, defaultdict\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Models and Voting Modes\n",
        "models_dict = {\n",
        "    \"Backbone Small-Dino\": backbone_smalldino,\n",
        "    \"Finetuned Small-Dino\": finetuned_smalldino\n",
        "}\n",
        "\n",
        "voting_modes = [\n",
        "    [\"WHT\"],\n",
        "    [\"WHT\", \"940\"],\n",
        "    [\"WHT\", \"940\", \"700\"],\n",
        "    [\"WHT\", \"940\", \"700\", \"460\"]\n",
        "]\n",
        "\n",
        "# Robust loader settings to stop the AssertionError\n",
        "loader_kwargs = {\n",
        "    'batch_size': 64,\n",
        "    'num_workers': 8,           # Reduced slightly for stability in notebooks\n",
        "    'pin_memory': True,\n",
        "    'persistent_workers': True,  # Keep workers alive to prevent shutdown errors\n",
        "    'shuffle': False\n",
        "}\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_embeddings(model, loader):\n",
        "    feats = defaultdict(list)\n",
        "    for imgs, labels, _ in tqdm(loader, desc=\"    Extracting\", leave=False):\n",
        "        imgs = imgs.to(device)\n",
        "        emb = model(imgs)\n",
        "        emb = F.normalize(emb, dim=1)\n",
        "        for e, l in zip(emb.cpu(), labels):\n",
        "            feats[l].append(e)\n",
        "    return feats\n",
        "\n",
        "# ========================================\n",
        "# Optimized Evaluation Script\n",
        "# ========================================\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    print(f\"\\n{'='*60}\\nSTARTING EVALUATION: {name}\\n{'='*60}\")\n",
        "    model.to(device).eval()\n",
        "    model_results = {\"Model\": name}\n",
        "\n",
        "    # 1. Registration Extraction (Normal dictionary approach is fine here)\n",
        "    reg_loader = DataLoader(Subset(dataset, reg_idx), batch_size=64, num_workers=4, pin_memory=True, shuffle=False)\n",
        "    reg_feats_dict = extract_embeddings(model, reg_loader)\n",
        "    RegMat, reg_labels = flatten_feats(reg_feats_dict)\n",
        "    RegMat = RegMat.to(device)\n",
        "\n",
        "    # 2. Query Extraction (Mapping back to ABSOLUTE indices)\n",
        "    print(\"--> Pre-extracting Query Pool (Mapping Absolute Indices)...\")\n",
        "    qry_loader = DataLoader(Subset(dataset, qry_idx), batch_size=64, num_workers=4, pin_memory=True, shuffle=False)\n",
        "\n",
        "    query_lookup = {}\n",
        "    current_pos = 0\n",
        "    for imgs, _, _ in tqdm(qry_loader, desc=\"    GPU Inference\", leave=False):\n",
        "        with torch.no_grad():\n",
        "            embs = model(imgs.to(device))\n",
        "            embs = F.normalize(embs, dim=1).cpu()\n",
        "            for i in range(embs.size(0)):\n",
        "                # Map the subset position back to the absolute index in dataset.samples\n",
        "                absolute_idx = qry_idx[current_pos]\n",
        "                query_lookup[absolute_idx] = embs[i]\n",
        "                current_pos += 1\n",
        "\n",
        "    # 3. Voting Phase\n",
        "    print(f\"    --- Starting Majority Voting ---\")\n",
        "    for mode in voting_modes:\n",
        "        mode_label = \"+\".join(mode)\n",
        "        all_true, all_pred = [], []\n",
        "\n",
        "        for hand_id in tqdm(hand_session_groups, desc=f\"    Voting [{mode_label}]\", leave=False):\n",
        "            for sess in [5, 6]:\n",
        "                specs_available = hand_session_groups[hand_id].get(sess, {})\n",
        "                votes = []\n",
        "\n",
        "                for spec_name in mode:\n",
        "                    # q_idx here is the ABSOLUTE index stored during your splitting step\n",
        "                    q_idx = specs_available.get(spec_name)\n",
        "                    if q_idx is not None:\n",
        "                        q_emb_cpu = query_lookup.get(q_idx)\n",
        "                        if q_emb_cpu is not None:\n",
        "                            sim = q_emb_cpu.to(device) @ RegMat.T\n",
        "                            votes.append(reg_labels[torch.argmax(sim).item()])\n",
        "\n",
        "                if votes:\n",
        "                    all_pred.append(Counter(votes).most_common(1)[0][0])\n",
        "                    all_true.append(hand_id)\n",
        "\n",
        "        acc = accuracy_score(all_true, all_pred) * 100 if all_true else 0\n",
        "        model_results[mode_label] = f\"{acc:.2f}%\"\n",
        "        print(f\"    [RESULT] {mode_label:20s}: {acc:.2f}%\")\n",
        "\n",
        "    results.append(model_results)\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"#\"*40 + \"\\n  FINAL RESULTS\\n\" + \"#\"*40)\n",
        "print(df_results.to_string(index=False))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IatWuNXEHJvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Verification**"
      ],
      "metadata": {
        "id": "El_J1GtQS-Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Palmprint Verification (EER) â€“ CASIA-MS FULL\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Paths & model\n",
        "# ----------------------------\n",
        "ROOT_DIR = \"/content/drive/MyDrive/CASIA-MS-ROI\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "backbone.eval()\n",
        "backbone.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Transform\n",
        "# ----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# Parse CASIA-MS images\n",
        "# Label = subject_hand (e.g., 001_l)\n",
        "# ----------------------------\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for root, _, files in os.walk(ROOT_DIR):\n",
        "    for fname in files:\n",
        "        if not fname.lower().endswith(\".jpg\"):\n",
        "            continue\n",
        "\n",
        "        # Expected: ID_{l|r}_{spectrum}_{iteration}.jpg\n",
        "        parts = fname[:-4].split(\"_\")\n",
        "        if len(parts) != 4:\n",
        "            continue\n",
        "\n",
        "        subject_id, hand, spectrum, iteration = parts\n",
        "        if hand not in [\"l\", \"r\"]:\n",
        "            continue\n",
        "\n",
        "        label = f\"{subject_id}_{hand}\"\n",
        "        img_path = os.path.join(root, fname)\n",
        "\n",
        "        images.append(img_path)\n",
        "        labels.append(label)\n",
        "\n",
        "print(f\"Total images: {len(images)}\")\n",
        "print(f\"Total unique hands: {len(set(labels))}\")\n",
        "\n",
        "assert len(images) > 0, \"No images found â€” check dataset path or filename format\"\n",
        "\n",
        "# ----------------------------\n",
        "# Extract embeddings\n",
        "# ----------------------------\n",
        "embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_path in images:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        emb = backbone(img)          # [1, D] ################################################\n",
        "        emb = F.normalize(emb, dim=1)\n",
        "\n",
        "        embeddings.append(emb.cpu())\n",
        "\n",
        "embeddings = torch.cat(embeddings, dim=0)  # [N, D]\n",
        "\n",
        "# ----------------------------\n",
        "# Genuine / Impostor matching\n",
        "# ----------------------------\n",
        "genuine_scores = []\n",
        "impostor_scores = []\n",
        "\n",
        "N = len(labels)\n",
        "\n",
        "for i in range(N):\n",
        "    for j in range(i + 1, N):\n",
        "        sim = torch.dot(embeddings[i], embeddings[j]).item()\n",
        "\n",
        "        if labels[i] == labels[j]:\n",
        "            genuine_scores.append(sim)\n",
        "        else:\n",
        "            impostor_scores.append(sim)\n",
        "\n",
        "print(f\"Genuine pairs: {len(genuine_scores)}\")\n",
        "print(f\"Impostor pairs: {len(impostor_scores)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# EER computation\n",
        "# ----------------------------\n",
        "scores = np.array(genuine_scores + impostor_scores)\n",
        "y_true = np.array([1] * len(genuine_scores) + [0] * len(impostor_scores))\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "fnr = 1 - tpr\n",
        "\n",
        "eer_idx = np.argmin(np.abs(fpr - fnr))\n",
        "eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "\n",
        "print(f\"\\n Equal Error Rate (EER): {eer*100:.2f}%\")\n",
        "\n",
        "# ----------------------------\n",
        "# Score distribution\n",
        "# ----------------------------\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.hist(genuine_scores, bins=50, alpha=0.6, label=\"Genuine\")\n",
        "plt.hist(impostor_scores, bins=50, alpha=0.6, label=\"Impostor\")\n",
        "plt.xlabel(\"Cosine Similarity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"CASIA-MS Palmprint Verification Score Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DkN1S_TueV1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Palmprint Verification (EER) â€“ CORRECT PARSER\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Paths & model\n",
        "# ----------------------------\n",
        "ROOT_DIR = \"/content/drive/MyDrive/palmprint_SIT/users-roi\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "backbone.eval()\n",
        "backbone.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Transform\n",
        "# ----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# Load images and labels\n",
        "# label = subject_hand (e.g., 03_l)\n",
        "# ----------------------------\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for root, _, files in os.walk(ROOT_DIR):\n",
        "    folder = os.path.basename(root)\n",
        "\n",
        "    # Expect folder like: 03_S1\n",
        "    if \"_\" not in folder:\n",
        "        continue\n",
        "\n",
        "    subject_id = folder.split(\"_\")[0]\n",
        "\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "            continue\n",
        "\n",
        "        fname = f.lower()\n",
        "\n",
        "        # ---- Hand from filename\n",
        "        if \"_left_\" in fname:\n",
        "            hand = \"l\"\n",
        "        elif \"_right_\" in fname:\n",
        "            hand = \"r\"\n",
        "        else:\n",
        "            continue  # skip malformed names\n",
        "\n",
        "        img_path = os.path.join(root, f)\n",
        "        label = f\"{subject_id}_{hand}\"\n",
        "\n",
        "        images.append(img_path)\n",
        "        labels.append(label)\n",
        "\n",
        "print(f\"Total images: {len(images)}\")\n",
        "print(f\"Total unique hands: {len(set(labels))}\")\n",
        "\n",
        "assert len(images) > 0, \"No images found â€” check filename format\"\n",
        "\n",
        "# ----------------------------\n",
        "# Extract embeddings\n",
        "# ----------------------------\n",
        "embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_path in images:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = transform(img).unsqueeze(0).to(device)\n",
        "        emb = backbone(img) ###########################################################\n",
        "        emb = F.normalize(emb, dim=1)\n",
        "        embeddings.append(emb.cpu())\n",
        "\n",
        "embeddings = torch.cat(embeddings, dim=0)  # [N, D]\n",
        "\n",
        "# ----------------------------\n",
        "# Genuine / Impostor matching\n",
        "# ----------------------------\n",
        "genuine_scores = []\n",
        "impostor_scores = []\n",
        "\n",
        "N = len(labels)\n",
        "\n",
        "for i in range(N):\n",
        "    for j in range(i + 1, N):\n",
        "        sim = torch.dot(embeddings[i], embeddings[j]).item()\n",
        "        if labels[i] == labels[j]:\n",
        "            genuine_scores.append(sim)\n",
        "        else:\n",
        "            impostor_scores.append(sim)\n",
        "\n",
        "print(f\"Genuine pairs: {len(genuine_scores)}\")\n",
        "print(f\"Impostor pairs: {len(impostor_scores)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# EER computation\n",
        "# ----------------------------\n",
        "scores = np.array(genuine_scores + impostor_scores)\n",
        "y_true = np.array([1]*len(genuine_scores) + [0]*len(impostor_scores))\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "fnr = 1 - tpr\n",
        "\n",
        "eer_idx = np.argmin(np.abs(fpr - fnr))\n",
        "eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "\n",
        "print(f\"\\n Equal Error Rate (EER): {eer*100:.2f}%\")\n",
        "\n",
        "# ----------------------------\n",
        "# Score distribution\n",
        "# ----------------------------\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.hist(genuine_scores, bins=50, alpha=0.6, label=\"Genuine\")\n",
        "plt.hist(impostor_scores, bins=50, alpha=0.6, label=\"Impostor\")\n",
        "plt.xlabel(\"Cosine Similarity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Palmprint Verification Score Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oNGBB4c9TF4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROI Extraction**"
      ],
      "metadata": {
        "id": "3hnf4akZOOyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.13"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aJKED5mwOVCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# google_roi.py\n",
        "\"\"\"\n",
        "Hand-ROI extractor based on MediaPipe Hands (single hand, 21 landmarks).\n",
        "\n",
        "Public API\n",
        "----------\n",
        "extract_palm_roi(image: numpy.ndarray) -> (roi_bgr, annotated_bgr, hand_type)\n",
        "\n",
        "â€¢ roi_bgr          â€“ cropped palm ROI (BGR)\n",
        "â€¢ annotated_bgr    â€“ same size as input with landmarks & box drawn\n",
        "â€¢ hand_type        â€“ \"Left\" / \"Right\" / \"Unknown\"\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Internal helpers\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def _run_mp_hands(image, min_det_conf=0.2):\n",
        "    mp_hands = mp.solutions.hands\n",
        "    with mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=1,\n",
        "        min_detection_confidence=min_det_conf\n",
        "    ) as hands:\n",
        "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        res = hands.process(rgb)\n",
        "        if not res.multi_hand_landmarks:\n",
        "            return None, None\n",
        "\n",
        "        lm = res.multi_hand_landmarks[0]\n",
        "        handedness = (\n",
        "            res.multi_handedness[0].classification[0].label\n",
        "            if res.multi_handedness else \"Unknown\"\n",
        "        )\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        pts = [(int(p.x * w), int(p.y * h)) for p in lm.landmark]\n",
        "        return pts, handedness\n",
        "\n",
        "\n",
        "def _midpoints(pairs):\n",
        "    return [((p1[0] + p2[0]) / 2, (p1[1] + p2[1]) / 2) for p1, p2 in pairs]\n",
        "\n",
        "\n",
        "def _calculate_point_c(m1, m2, thumb):\n",
        "    m1, m2, thumb = map(np.asarray, (m1, m2, thumb))\n",
        "\n",
        "    O = (m1 + m2) / 2.0\n",
        "    AB = m2 - m1\n",
        "    L = np.linalg.norm(AB)\n",
        "    if L == 0:\n",
        "        raise ValueError(\"Midpoints coincide\")\n",
        "\n",
        "    ABu = AB / L\n",
        "    perp = np.array([-ABu[1], ABu[0]])\n",
        "\n",
        "    # ---- NumPy 2.0â€“safe 2D cross product (scalar z-component)\n",
        "    cross_z = ABu[0] * (thumb - O)[1] - ABu[1] * (thumb - O)[0]\n",
        "    if cross_z < 0:\n",
        "        perp = -perp\n",
        "\n",
        "    C = O + 1.8 * L * perp\n",
        "    return int(C[0]), int(C[1])  # ensure Python ints\n",
        "\n",
        "\n",
        "def _extract_roi(img, mid1, mid2, C, thumb, hand_type):\n",
        "    vec = np.array(mid2) - np.array(mid1)\n",
        "    angle = np.degrees(np.arctan2(vec[1], vec[0]))\n",
        "\n",
        "    C = (int(C[0]), int(C[1]))  # OpenCV-safe center\n",
        "\n",
        "    if hand_type.lower() == \"right\":\n",
        "        if np.dot(vec, np.array(thumb) - np.array(C)) > 0:\n",
        "            angle += 180\n",
        "    else:  # left / unknown\n",
        "        if np.dot(vec, np.array(thumb) - np.array(C)) < 0:\n",
        "            angle += 180\n",
        "\n",
        "    side = np.linalg.norm(vec) * 2.5\n",
        "    rect = (C, (side, side), angle)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D(C, angle, 1.0)\n",
        "    rot = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
        "    roi = cv2.getRectSubPix(rot, (int(side), int(side)), C)\n",
        "\n",
        "    box = cv2.boxPoints(rect).astype(int)\n",
        "    return roi, box\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Public function\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def extract_palm_roi(image_bgr):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_bgr : np.ndarray\n",
        "        BGR image.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    roi_bgr, annotated_bgr, hand_type\n",
        "        If landmarks fail â†’ returns (None, None, None)\n",
        "    \"\"\"\n",
        "    lms, hand_type = _run_mp_hands(image_bgr)\n",
        "    if lms is None:\n",
        "        return None, None, None\n",
        "\n",
        "    idx = lambda i: lms[i]\n",
        "\n",
        "    mids4 = _midpoints([\n",
        "        (idx(17), idx(18)),\n",
        "        (idx(14), idx(13)),\n",
        "        (idx(10), idx(9)),\n",
        "        (idx(6),  idx(5))\n",
        "    ])\n",
        "\n",
        "    adj = _midpoints([\n",
        "        (mids4[0], mids4[1]),\n",
        "        (mids4[1], mids4[2]),\n",
        "        (mids4[2], mids4[3])\n",
        "    ])\n",
        "\n",
        "    roi_mid1 = ((adj[0][0] + adj[1][0]) / 2,\n",
        "                (adj[0][1] + adj[1][1]) / 2)\n",
        "\n",
        "    roi_mid2 = ((adj[1][0] + adj[2][0]) / 2,\n",
        "                (adj[1][1] + adj[2][1]) / 2)\n",
        "\n",
        "    thumb = idx(2)\n",
        "    C = _calculate_point_c(roi_mid1, roi_mid2, thumb)\n",
        "\n",
        "    roi, box = _extract_roi(image_bgr, roi_mid1, roi_mid2, C, thumb, hand_type)\n",
        "\n",
        "    # Annotated image\n",
        "    ann = image_bgr.copy()\n",
        "    for x, y in lms:\n",
        "        cv2.circle(ann, (x, y), 3, (0, 255, 0), -1)\n",
        "\n",
        "    cv2.polylines(ann, [box], True, (0, 255, 0), 2)\n",
        "    cv2.circle(ann, C, 6, (0, 0, 255), -1)\n",
        "\n",
        "    return roi, ann, hand_type\n"
      ],
      "metadata": {
        "id": "Q-d3lpHJOSkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ================================\n",
        "# Paths\n",
        "# ================================\n",
        "SRC_ROOT = \"/content/drive/MyDrive/palmprint_SIT/users\"\n",
        "DST_ROOT = \"/content/drive/MyDrive/palmprint_SIT/users-roi\"\n",
        "\n",
        "os.makedirs(DST_ROOT, exist_ok=True)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Collect all images\n",
        "# ================================\n",
        "all_images = []\n",
        "for root, _, files in os.walk(SRC_ROOT):\n",
        "    for f in files:\n",
        "        if f.lower().endswith(\".jpg\"):\n",
        "            all_images.append(os.path.join(root, f))\n",
        "\n",
        "print(f\"Total images found: {len(all_images)}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Counters\n",
        "# ================================\n",
        "num_success = 0\n",
        "num_failed = 0\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Batch processing\n",
        "# ================================\n",
        "failed_images = []\n",
        "for src_path in tqdm(all_images):\n",
        "    rel_path = os.path.relpath(src_path, SRC_ROOT)\n",
        "    dst_path = os.path.join(DST_ROOT, rel_path)\n",
        "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "\n",
        "    # ---- Load image (RGB)\n",
        "    img_rgb = np.array(Image.open(src_path).convert(\"RGB\"))\n",
        "\n",
        "    # ---- Convert to BGR for ROI extractor\n",
        "    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # ---- Extract palm ROI (advanced method)\n",
        "    roi_bgr, _, _ = extract_palm_roi(img_bgr)\n",
        "\n",
        "    # ---- Check extraction result\n",
        "    if roi_bgr is None:\n",
        "        num_failed += 1\n",
        "        failed_images.append(src_path)  # <-- save path\n",
        "    else:\n",
        "        num_success += 1\n",
        "        roi_bgr = cv2.resize(roi_bgr, (160, 160))\n",
        "        # ---- Save ROI\n",
        "        cv2.imwrite(dst_path, roi_bgr)\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Final report\n",
        "# ================================\n",
        "print(\"ROI extraction completed.\")\n",
        "print(f\"Successfully extracted ROIs : {num_success}\")\n",
        "print(f\"Failed extractions (fallback used): {num_failed}\")\n",
        "print(f\"Success rate: {100.0 * num_success / len(all_images):.2f}%\")"
      ],
      "metadata": {
        "id": "a6dXK67uOlSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual ROI Extraction**"
      ],
      "metadata": {
        "id": "S4dMIm0OV-Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jxPYKyi6RMvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def select_roi(image):\n",
        "    pts = []\n",
        "\n",
        "    def onclick(event):\n",
        "        if event.xdata is None or event.ydata is None:\n",
        "            return\n",
        "\n",
        "        pts.append((int(event.xdata), int(event.ydata)))\n",
        "        print(f\"Point {len(pts)}: {pts[-1]}\")\n",
        "\n",
        "        if len(pts) == 2:\n",
        "            plt.close()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(\"Click TOP-LEFT then BOTTOM-RIGHT\")\n",
        "    cid = fig.canvas.mpl_connect(\"button_press_event\", onclick)\n",
        "\n",
        "    # ðŸ”’ BLOCK until figure is closed\n",
        "    plt.show(block=True)\n",
        "\n",
        "    if len(pts) != 2:\n",
        "        return None\n",
        "\n",
        "    (x1, y1), (x2, y2) = pts\n",
        "    x1, x2 = sorted([x1, x2])\n",
        "    y1, y2 = sorted([y1, y2])\n",
        "    return x1, y1, x2, y2\n"
      ],
      "metadata": {
        "id": "mIMwEnO2Rl-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import IntSlider, Button, VBox, Output, Label, interactive_output\n",
        "\n",
        "# ================================\n",
        "# Paths\n",
        "# ================================\n",
        "input_root = \"/content/drive/MyDrive/palmprint_SIT/users\"\n",
        "output_root = \"/content/drive/MyDrive/palmprint_SIT/users_roi\"\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "\n",
        "\n",
        "\n",
        "# Use only failed images\n",
        "# failed_images should be a list of full paths\n",
        "# Convert failed_images to tuple (src_path, dst_path)\n",
        "images = []\n",
        "for src_path in failed_images:\n",
        "    rel_path = os.path.relpath(src_path, input_root)\n",
        "    dst_path = os.path.join(output_root, rel_path)\n",
        "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "    images.append((src_path, dst_path))\n",
        "\n",
        "print(f\"Processing {len(images)} failed images manually.\")\n",
        "\n",
        "# ================================\n",
        "# Widgets\n",
        "# ================================\n",
        "out = Output()\n",
        "label_widget = Label()\n",
        "save_button = Button(description=\"Save Crop & Next\")\n",
        "\n",
        "current_idx = 0\n",
        "img_np = None\n",
        "\n",
        "x1_slider = IntSlider(description='x1')\n",
        "y1_slider = IntSlider(description='y1')\n",
        "x2_slider = IntSlider(description='x2')\n",
        "y2_slider = IntSlider(description='y2')\n",
        "\n",
        "# ================================\n",
        "# Function to update display\n",
        "# ================================\n",
        "def update_display(x1, y1, x2, y2):\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        x1_, x2_ = sorted([max(0, x1), min(img_np.shape[1], x2)])\n",
        "        y1_, y2_ = sorted([max(0, y1), min(img_np.shape[0], y2)])\n",
        "        if x2_ <= x1_ or y2_ <= y1_:\n",
        "            plt.figure(figsize=(6,6))\n",
        "            plt.imshow(img_np)\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Invalid ROI\")\n",
        "            plt.show()\n",
        "            return\n",
        "        crop = img_np[y1_:y2_, x1_:x2_]\n",
        "        plt.figure(figsize=(6,6))\n",
        "        plt.imshow(crop)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# ================================\n",
        "# Save crop and move to next\n",
        "# ================================\n",
        "def save_crop(b):\n",
        "    global current_idx\n",
        "    if current_idx >= len(images):\n",
        "        return\n",
        "\n",
        "    x1, y1 = x1_slider.value, y1_slider.value\n",
        "    x2, y2 = x2_slider.value, y2_slider.value\n",
        "    x1, x2 = sorted([max(0, x1), min(img_np.shape[1], x2)])\n",
        "    y1, y2 = sorted([max(0, y1), min(img_np.shape[0], y2)])\n",
        "\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        print(\"Invalid crop, adjust sliders!\")\n",
        "        return\n",
        "\n",
        "    crop = img_np[y1:y2, x1:x2]\n",
        "    src_path, save_path = images[current_idx]\n",
        "    Image.fromarray(crop).save(save_path)\n",
        "    print(f\"Saved â†’ {save_path}\")\n",
        "\n",
        "    current_idx += 1\n",
        "    display_image(current_idx)\n",
        "\n",
        "save_button.on_click(save_crop)\n",
        "\n",
        "# ================================\n",
        "# Display image and set sliders\n",
        "# ================================\n",
        "def display_image(idx):\n",
        "    global img_np\n",
        "    if idx >= len(images):\n",
        "        label_widget.value = \"All failed images processed!\"\n",
        "        out.clear_output()\n",
        "        return\n",
        "\n",
        "    img_path, _ = images[idx]\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_np = np.array(img)\n",
        "    h, w, _ = img_np.shape\n",
        "\n",
        "    # Set slider ranges\n",
        "    x1_slider.min, x1_slider.max, x1_slider.value = 0, w-1, 0\n",
        "    y1_slider.min, y1_slider.max, y1_slider.value = 0, h-1, 0\n",
        "    x2_slider.min, x2_slider.max, x2_slider.value = 1, w, w\n",
        "    y2_slider.min, y2_slider.max, y2_slider.value = 1, h, h\n",
        "\n",
        "    label_widget.value = f\"Processing {idx+1}/{len(images)}: {os.path.basename(img_path)}\"\n",
        "\n",
        "    interactive_output(update_display, {'x1': x1_slider, 'y1': y1_slider,\n",
        "                                       'x2': x2_slider, 'y2': y2_slider})\n",
        "    update_display(x1_slider.value, y1_slider.value, x2_slider.value, y2_slider.value)\n",
        "\n",
        "# ================================\n",
        "# Layout\n",
        "# ================================\n",
        "ui = VBox([label_widget, x1_slider, y1_slider, x2_slider, y2_slider, save_button, out])\n",
        "\n",
        "# Start with first failed image\n",
        "display_image(current_idx)\n",
        "ui\n"
      ],
      "metadata": {
        "id": "aQxul3s2Rst-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}